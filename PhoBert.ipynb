{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaiKhoa0101/MachineLearning/blob/main/PhoBert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EO4rL7Ootqm0"
      },
      "outputs": [],
      "source": [
        "!pip install neo4j -q\n",
        "!pip install Cython\n",
        "!pip install -U fasttext --no-cache-dir --no-deps --force-reinstall\n",
        "# ==================== C√ÄI ƒê·∫∂T ====================\n",
        "# Ch·∫°y cell n√†y tr∆∞·ªõc\n",
        "!pip install -q flask flask-cors pyngrok transformers torch\n",
        "\n",
        "\n",
        "print(\"‚úÖ ƒê√£ c√†i ƒë·∫∑t xong c√°c th∆∞ vi·ªán\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CLaqO4vttude",
        "outputId": "8b47a811-5e02-4aa5-8fab-7b117cee2640"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üñ•Ô∏è  Device: cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "from neo4j import GraphDatabase\n",
        "import numpy as np\n",
        "from typing import List, Dict, Tuple\n",
        "import warnings\n",
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Check GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"üñ•Ô∏è  Device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8IZrxvstziM"
      },
      "outputs": [],
      "source": [
        "print(\"üì• ƒêang t·∫£i PhoBERT-large...\")\n",
        "\n",
        "# Load tokenizer v√† model\n",
        "MODEL_NAME = \"vinai/phobert-large\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForMaskedLM.from_pretrained(MODEL_NAME)\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "print(\"‚úÖ ƒê√£ load PhoBERT-large th√†nh c√¥ng!\")\n",
        "print(f\"   Vocab size: {tokenizer.vocab_size:,}\")\n",
        "print(f\"   Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRYHDP_Gt7iV",
        "outputId": "4840d6b4-cf49-45cc-8d1b-81ca79c308aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üõë ƒêang reset Ngrok...\n",
            "‚úÖ ƒê√£ reset Ngrok local\n",
            "\n",
            "‚è≥ ƒêang t·∫£i PhoBERT model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at vinai/phobert-large were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ ƒê√£ t·∫£i xong PhoBERT! (Device: cpu)\n",
            "\n",
            "üîå ƒêang k·∫øt n·ªëi Ngrok...\n",
            "\n",
            "======================================================================\n",
            "‚úÖ PHOBERT SERVER ƒêANG CH·∫†Y TH√ÄNH C√îNG!\n",
            "======================================================================\n",
            "üì° Public URL:  https://veinless-unslanderously-jordyn.ngrok-free.dev\n",
            "üè• Health Check: https://veinless-unslanderously-jordyn.ngrok-free.dev/health\n",
            "üéØ Predict API:  https://veinless-unslanderously-jordyn.ngrok-free.dev/predict\n",
            "======================================================================\n",
            "\n",
            "üí° Tip: URL n√†y s·∫Ω thay ƒë·ªïi m·ªói l·∫ßn kh·ªüi ƒë·ªông l·∫°i\n",
            "    N·∫øu mu·ªën domain c·ªë ƒë·ªãnh, n√¢ng c·∫•p t√†i kho·∫£n Ngrok\n",
            "\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:5010\n",
            " * Running on http://172.28.0.12:5010\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import torch\n",
        "import warnings\n",
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "from pyngrok import ngrok\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "\n",
        "# ==================== 1. RESET NGROK ====================\n",
        "print(\"üõë ƒêang reset Ngrok...\")\n",
        "\n",
        "try:\n",
        "    os.system(\"killall ngrok 2>/dev/null\")  # T·∫Øt process local\n",
        "    ngrok.kill()  # T·∫Øt qua th∆∞ vi·ªán\n",
        "    time.sleep(3)  # ƒê·ª£i cleanup\n",
        "    print(\"‚úÖ ƒê√£ reset Ngrok local\")\n",
        "except:\n",
        "    print(\"‚ö†Ô∏è  Kh√¥ng c√≥ Ngrok process n√†o ƒëang ch·∫°y\")\n",
        "\n",
        "# ==================== 2. LOAD MODEL ====================\n",
        "print(\"\\n‚è≥ ƒêang t·∫£i PhoBERT model...\")\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-large\")\n",
        "    model = AutoModelForMaskedLM.from_pretrained(\"vinai/phobert-large\")\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    print(f\"‚úÖ ƒê√£ t·∫£i xong PhoBERT! (Device: {device})\\n\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå L·ªói t·∫£i model: {e}\")\n",
        "    exit(1)\n",
        "\n",
        "# ==================== 3. FLASK APP ====================\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "\n",
        "@app.route('/health', methods=['GET'])\n",
        "def health():\n",
        "    \"\"\"Health check endpoint\"\"\"\n",
        "    return jsonify({\n",
        "        'status': 'ok',\n",
        "        'model': 'PhoBERT-large',\n",
        "        'device': str(device),\n",
        "        'vocab_size': tokenizer.vocab_size\n",
        "    })\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    \"\"\"\n",
        "    Word prediction endpoint\n",
        "\n",
        "    Request body:\n",
        "    {\n",
        "        \"context\": \"T√¥i ƒëang\",\n",
        "        \"candidates\": [\"h·ªçc\", \"l√†m\", \"ƒÉn\", \"ng·ªß\"],\n",
        "        \"top_k\": 5\n",
        "    }\n",
        "    \"\"\"\n",
        "    try:\n",
        "        data = request.json\n",
        "        context = data.get('context', '').strip()\n",
        "        candidates = data.get('candidates', [])\n",
        "        top_k = data.get('top_k', 10)\n",
        "\n",
        "        # Validation\n",
        "        if not context:\n",
        "            return jsonify({\n",
        "                'success': False,\n",
        "                'message': 'Context kh√¥ng ƒë∆∞·ª£c r·ªóng'\n",
        "            }), 400\n",
        "\n",
        "        if not candidates:\n",
        "            return jsonify({\n",
        "                'success': False,\n",
        "                'message': 'Candidates kh√¥ng ƒë∆∞·ª£c r·ªóng'\n",
        "            }), 400\n",
        "\n",
        "        # Th√™m mask token\n",
        "        masked_text = f\"{context} {tokenizer.mask_token}\"\n",
        "\n",
        "        # Tokenize\n",
        "        inputs = tokenizer(masked_text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "        # T√¨m v·ªã tr√≠ mask\n",
        "        mask_indices = torch.where(\n",
        "            inputs[\"input_ids\"] == tokenizer.mask_token_id\n",
        "        )[1]\n",
        "\n",
        "        if len(mask_indices) == 0:\n",
        "            return jsonify({\n",
        "                'success': False,\n",
        "                'message': 'Kh√¥ng t√¨m th·∫•y v·ªã tr√≠ mask token'\n",
        "            }), 400\n",
        "\n",
        "        mask_token_index = mask_indices[0]\n",
        "\n",
        "        # D·ª± ƒëo√°n\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            logits = outputs.logits\n",
        "\n",
        "        # T√≠nh probabilities\n",
        "        mask_logits = logits[0, mask_token_index, :]\n",
        "        probs = torch.softmax(mask_logits, dim=-1)\n",
        "\n",
        "        # Score t·ª´ng candidate\n",
        "        predictions = []\n",
        "        for candidate in candidates:\n",
        "            # Tokenize candidate (l·∫•y token ƒë·∫ßu ti√™n)\n",
        "            tokens = tokenizer.encode(\n",
        "                candidate,\n",
        "                add_special_tokens=False\n",
        "            )\n",
        "\n",
        "            if len(tokens) > 0:\n",
        "                token_id = tokens[0]\n",
        "                score = probs[token_id].item()\n",
        "                predictions.append({\n",
        "                    \"word\": candidate,\n",
        "                    \"score\": float(score),\n",
        "                    \"token_id\": int(token_id)\n",
        "                })\n",
        "\n",
        "        # Sort theo score gi·∫£m d·∫ßn\n",
        "        predictions.sort(key=lambda x: x['score'], reverse=True)\n",
        "\n",
        "        return jsonify({\n",
        "            'success': True,\n",
        "            'context': context,\n",
        "            'masked_text': masked_text,\n",
        "            'predictions': predictions[:top_k],\n",
        "            'total_candidates': len(candidates)\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå L·ªói x·ª≠ l√Ω request: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return jsonify({\n",
        "            'success': False,\n",
        "            'message': str(e)\n",
        "        }), 500\n",
        "\n",
        "# ==================== 4. RUN SERVER ====================\n",
        "if __name__ == '__main__':\n",
        "    NGROK_TOKEN = \"37ji3NBafQsRSv0DACn5Vkf8mFD_HramkUED3pcgezorryEF\"\n",
        "    ngrok.set_auth_token(NGROK_TOKEN)\n",
        "\n",
        "    PORT = 5010\n",
        "\n",
        "    try:\n",
        "        print(\"üîå ƒêang k·∫øt n·ªëi Ngrok...\")\n",
        "\n",
        "        # FIX: Kh√¥ng ch·ªâ ƒë·ªãnh domain ƒë·ªÉ tr√°nh conflict\n",
        "        # Ngrok s·∫Ω t·ª± ƒë·ªông t·∫°o domain ng·∫´u nhi√™n m·ªói l·∫ßn\n",
        "        public_url_obj = ngrok.connect(\n",
        "            PORT,\n",
        "            bind_tls=True  # Ch·ªâ d√πng HTTPS\n",
        "        )\n",
        "\n",
        "        public_url = public_url_obj.public_url\n",
        "\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"‚úÖ PHOBERT SERVER ƒêANG CH·∫†Y TH√ÄNH C√îNG!\")\n",
        "        print(\"=\"*70)\n",
        "        print(f\"üì° Public URL:  {public_url}\")\n",
        "        print(f\"üè• Health Check: {public_url}/health\")\n",
        "        print(f\"üéØ Predict API:  {public_url}/predict\")\n",
        "        print(\"=\"*70)\n",
        "        print(\"\\nüí° Tip: URL n√†y s·∫Ω thay ƒë·ªïi m·ªói l·∫ßn kh·ªüi ƒë·ªông l·∫°i\")\n",
        "        print(\"    N·∫øu mu·ªën domain c·ªë ƒë·ªãnh, n√¢ng c·∫•p t√†i kho·∫£n Ngrok\\n\")\n",
        "\n",
        "        # Ch·∫°y Flask\n",
        "        app.run(host='0.0.0.0', port=PORT, debug=False)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"‚ùå L·ªñI KH·ªûI ƒê·ªòNG NGROK\")\n",
        "        print(\"=\"*70)\n",
        "        print(f\"Chi ti·∫øt: {str(e)}\\n\")\n",
        "\n",
        "        if \"already online\" in str(e):\n",
        "            print(\"üîß C√ÅCH FIX:\")\n",
        "            print(\"   1. V√†o: https://dashboard.ngrok.com/agents/sessions\")\n",
        "            print(\"   2. T√¨m session ƒëang ch·∫°y v√† b·∫•m 'Stop'\")\n",
        "            print(\"   3. Ch·∫°y l·∫°i script n√†y\")\n",
        "            print(\"\\n   Ho·∫∑c ƒë·ª£i 5-10 ph√∫t ƒë·ªÉ session t·ª± h·∫øt h·∫°n\\n\")\n",
        "\n",
        "        print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7lVVwImt3gt"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# T·∫Øt t·∫•t c·∫£ c√°c tunnel ƒëang ch·∫°y\n",
        "ngrok.kill()\n",
        "class PhoBERTNextWordPredictor:\n",
        "    \"\"\"Predictor s·ª≠ d·ª•ng PhoBERT ƒë·ªÉ d·ª± ƒëo√°n t·ª´ ti·∫øp theo\"\"\"\n",
        "\n",
        "    def __init__(self, model, tokenizer, device):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.device = device\n",
        "        self.mask_token = tokenizer.mask_token\n",
        "\n",
        "    def predict_next_word(self,\n",
        "                         context: str,\n",
        "                         top_k: int = 10,\n",
        "                         filter_words: List[str] = None) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        D·ª± ƒëo√°n t·ª´ ti·∫øp theo d·ª±a tr√™n context\n",
        "\n",
        "        Args:\n",
        "            context: C√¢u/c·ª•m t·ª´ hi·ªán t·∫°i (VD: \"T√¥i ƒëang\")\n",
        "            top_k: S·ªë l∆∞·ª£ng t·ª´ g·ª£i √Ω\n",
        "            filter_words: Danh s√°ch t·ª´ c·∫ßn filter (optional)\n",
        "\n",
        "        Returns:\n",
        "            List of {word, score, rank}\n",
        "        \"\"\"\n",
        "        # Th√™m mask token v√†o cu·ªëi context\n",
        "        masked_text = f\"{context} {self.mask_token}\"\n",
        "\n",
        "        # Tokenize\n",
        "        inputs = self.tokenizer(masked_text, return_tensors=\"pt\").to(self.device)\n",
        "\n",
        "        # Get mask token position\n",
        "        mask_token_index = torch.where(inputs[\"input_ids\"] == self.tokenizer.mask_token_id)[1]\n",
        "\n",
        "        # Predict\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "            logits = outputs.logits\n",
        "\n",
        "        # Get predictions for mask token\n",
        "        mask_token_logits = logits[0, mask_token_index, :]\n",
        "\n",
        "        # Convert to probabilities\n",
        "        probs = torch.softmax(mask_token_logits, dim=-1)\n",
        "\n",
        "        # Get top k predictions\n",
        "        top_k_probs, top_k_indices = torch.topk(probs, k=min(top_k * 3, 100), dim=-1)\n",
        "\n",
        "        # Decode tokens\n",
        "        predictions = []\n",
        "        for i in range(top_k_indices.shape[1]):\n",
        "            token_id = top_k_indices[0, i].item()\n",
        "            token = self.tokenizer.decode([token_id]).strip()\n",
        "            score = top_k_probs[0, i].item()\n",
        "\n",
        "            # Filter out special tokens v√† subwords\n",
        "            if token.startswith('##') or token in ['<s>', '</s>', '<pad>', '<unk>']:\n",
        "                continue\n",
        "\n",
        "            # Filter custom words if provided\n",
        "            if filter_words and token.lower() in [w.lower() for w in filter_words]:\n",
        "                continue\n",
        "\n",
        "            predictions.append({\n",
        "                'word': token,\n",
        "                'score': score,\n",
        "                'rank': len(predictions) + 1\n",
        "            })\n",
        "\n",
        "            if len(predictions) >= top_k:\n",
        "                break\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def predict_multiple_positions(self,\n",
        "                                   context: str,\n",
        "                                   num_words: int = 3,\n",
        "                                   top_k_per_position: int = 5) -> List[List[Dict]]:\n",
        "        \"\"\"\n",
        "        D·ª± ƒëo√°n nhi·ªÅu t·ª´ ti·∫øp theo (sequence prediction)\n",
        "\n",
        "        Returns:\n",
        "            List of predictions for each position\n",
        "        \"\"\"\n",
        "        all_predictions = []\n",
        "        current_context = context\n",
        "\n",
        "        for i in range(num_words):\n",
        "            predictions = self.predict_next_word(current_context, top_k=top_k_per_position)\n",
        "\n",
        "            if not predictions:\n",
        "                break\n",
        "\n",
        "            all_predictions.append(predictions)\n",
        "\n",
        "            # Extend context with best prediction\n",
        "            best_word = predictions[0]['word']\n",
        "            current_context = f\"{current_context} {best_word}\"\n",
        "\n",
        "        return all_predictions\n",
        "\n",
        "\n",
        "# Kh·ªüi t·∫°o predictor\n",
        "phobert_predictor = PhoBERTNextWordPredictor(model, tokenizer, device)\n",
        "print(\"‚úÖ PhoBERT Predictor ƒë√£ s·∫µn s√†ng!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ApASWB0t56F"
      },
      "outputs": [],
      "source": [
        "def print_predictions(predictions: List[Dict], title: str = \"Predictions\"):\n",
        "    \"\"\"In k·∫øt qu·∫£ d·ª± ƒëo√°n ƒë·∫πp m·∫Øt\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"üìä {title}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    for pred in predictions:\n",
        "        rank = pred.get('rank', '?')\n",
        "        word = pred['word']\n",
        "\n",
        "        if 'combined_score' in pred:\n",
        "            # Hybrid predictions\n",
        "            combined = pred['combined_score']\n",
        "            phobert = pred.get('phobert_score', 0)\n",
        "            neo4j = pred.get('neo4j_score', 0)\n",
        "            print(f\"{rank:2d}. {word:15s} | Combined: {combined:.4f} | PhoBERT: {phobert:.4f} | Neo4j: {neo4j:.4f}\")\n",
        "        else:\n",
        "            # Simple predictions\n",
        "            score = pred.get('score', 0)\n",
        "            print(f\"{rank:2d}. {word:15s} | Score: {score:.4f}\")\n",
        "\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "\n",
        "# ========== TEST CASES ==========\n",
        "\n",
        "test_sentences = [\n",
        "    \"H√¥m nay t√¥i mu·ªën ƒëi ch∆°i v·ªõi\",\n",
        "    \"H√¥m nay tr·ªùi\",\n",
        "    \"Anh ·∫•y r·∫•t\",\n",
        "    \"Ch√∫ng t√¥i s·∫Ω\",\n",
        "    \"Vi·ªát Nam l√†\",\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üß™ TESTING PHOBERT PREDICTOR\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    print(f\"\\nüìù Context: \\\"{sentence}\\\"\")\n",
        "    predictions = phobert_predictor.predict_next_word(sentence, top_k=5)\n",
        "    print_predictions(predictions, title=f\"PhoBERT Predictions for '{sentence}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGLE-2rQ1jk9"
      },
      "outputs": [],
      "source": [
        "# ==================== TEST PHOBERT API ====================\n",
        "# Ch·∫°y cell n√†y SAU KHI server ƒë√£ ch·∫°y\n",
        "\n",
        "import requests\n",
        "\n",
        "# ‚ö†Ô∏è THAY ƒê·ªîI URL n√†y b·∫±ng URL t·ª´ output c·ªßa cell tr∆∞·ªõc\n",
        "PUBLIC_URL = \"https://your-url-here.ngrok.io\"  # ‚Üê Thay ƒë·ªïi URL n√†y!\n",
        "\n",
        "print(\"üß™ Testing PhoBERT API...\")\n",
        "print(f\"üì° URL: {PUBLIC_URL}\\n\")\n",
        "\n",
        "# ==================== TEST 1: HEALTH CHECK ====================\n",
        "print(\"1Ô∏è‚É£ Testing /health...\")\n",
        "try:\n",
        "    response = requests.get(f\"{PUBLIC_URL}/health\")\n",
        "    result = response.json()\n",
        "    print(f\"   Status: {result['status']}\")\n",
        "    print(f\"   Model:  {result['model']}\")\n",
        "    print(f\"   Device: {result['device']}\")\n",
        "    print(\"   ‚úÖ Health check OK\\n\")\n",
        "except Exception as e:\n",
        "    print(f\"   ‚ùå Error: {e}\\n\")\n",
        "\n",
        "# ==================== TEST 2: PREDICT ====================\n",
        "print(\"2Ô∏è‚É£ Testing /predict...\")\n",
        "\n",
        "test_cases = [\n",
        "    {\n",
        "        \"context\": \"T√¥i ƒëang\",\n",
        "        \"candidates\": [\"h·ªçc\", \"l√†m\", \"·ªü\", \"nghƒ©\", \"ƒëi\"],\n",
        "        \"top_k\": 5\n",
        "    },\n",
        "    {\n",
        "        \"context\": \"H√¥m nay tr·ªùi\",\n",
        "        \"candidates\": [\"ƒë·∫πp\", \"x·∫•u\", \"n√≥ng\", \"m∆∞a\", \"l·∫°nh\"],\n",
        "        \"top_k\": 3\n",
        "    },\n",
        "    {\n",
        "        \"context\": \"T√¥i y√™u\",\n",
        "        \"candidates\": [\"b·∫°n\", \"em\", \"m·∫π\", \"cha\", \"anh\"],\n",
        "        \"top_k\": 3\n",
        "    }\n",
        "]\n",
        "\n",
        "for i, test in enumerate(test_cases, 1):\n",
        "    print(f\"\\n   Test {i}: '{test['context']}'\")\n",
        "    print(f\"   Candidates: {test['candidates']}\")\n",
        "\n",
        "    try:\n",
        "        response = requests.post(\n",
        "            f\"{PUBLIC_URL}/predict\",\n",
        "            json=test\n",
        "        )\n",
        "\n",
        "        result = response.json()\n",
        "\n",
        "        if result['success']:\n",
        "            print(f\"   Results:\")\n",
        "            for j, pred in enumerate(result['predictions'], 1):\n",
        "                print(f\"      {j}. {pred['word']:10s} ‚Üí {pred['score']:.4f}\")\n",
        "            print(f\"   ‚úÖ Test {i} passed\")\n",
        "        else:\n",
        "            print(f\"   ‚ùå Test {i} failed: {result.get('message')}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Error: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚úÖ Testing completed!\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ==================== USAGE EXAMPLE ====================\n",
        "print(\"\\nüìù S·ª≠ d·ª•ng trong NestJS:\")\n",
        "print(f\"\"\"\n",
        "// .env\n",
        "PHOBERT_API_URL={PUBLIC_URL}\n",
        "ENABLE_PHOBERT=true\n",
        "HYBRID_ALPHA=0.6\n",
        "\n",
        "// TypeScript\n",
        "const result = await nlpService.getNextWordSuggestion('T√¥i ƒëang');\n",
        "console.log(result.suggestions);\n",
        "\"\"\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}