{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaiKhoa0101/MachineLearning/blob/main/CODE_Detect.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade mediapipe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "X5sxNk8vHm1k",
        "outputId": "bf36de6c-d1bc-4e1c-a22e-034fe21607b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mediapipe in /usr/local/lib/python3.12/dist-packages (0.10.31)\n",
            "Requirement already satisfied: absl-py~=2.3 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (2.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from mediapipe) (2.0.2)\n",
            "Requirement already satisfied: sounddevice~=0.5 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.5.3)\n",
            "Requirement already satisfied: flatbuffers~=25.9 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (25.9.23)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.12/dist-packages (from sounddevice~=0.5->mediapipe) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from CFFI>=1.0->sounddevice~=0.5->mediapipe) (2.23)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "collapsed": true,
        "id": "LPT4alDGEY4y",
        "outputId": "0292f806-6c06-4ae1-8dda-3bc9dfd4d27c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'mediapipe' has no attribute 'solutions'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4178506015.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;31m# --------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m     \u001b[0mdetector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSignLanguageDetector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'detech.xlsx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m     \u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test_video.mp4'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'detected_gestures.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_video\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4178506015.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, excel_path)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexcel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'detech.xlsx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# MediaPipe Hands\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmp_hands\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolutions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhands\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         self.hands = self.mp_hands.Hands(\n\u001b[1;32m     19\u001b[0m             \u001b[0mstatic_image_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'mediapipe' has no attribute 'solutions'"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Sign Language Gesture Detection System\n",
        "Phát hiện động tác ngón tay từ video và xuất JSON\n",
        "\"\"\"\n",
        "\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import math\n",
        "\n",
        "\n",
        "class SignLanguageDetector:\n",
        "    def __init__(self, excel_path='detech.xlsx'):\n",
        "        # MediaPipe Hands\n",
        "        self.mp_hands = mp.solutions.hands\n",
        "        self.hands = self.mp_hands.Hands(\n",
        "            static_image_mode=False,\n",
        "            max_num_hands=2,\n",
        "            min_detection_confidence=0.7,\n",
        "            min_tracking_confidence=0.7\n",
        "        )\n",
        "        self.mp_drawing = mp.solutions.drawing_utils\n",
        "\n",
        "        # Load rules\n",
        "        self.rules = self.load_rules(excel_path)\n",
        "\n",
        "        # Finger landmarks\n",
        "        self.finger_landmarks = {\n",
        "            'thumb': [1, 2, 3, 4],\n",
        "            'index': [5, 6, 7, 8],\n",
        "            'middle': [9, 10, 11, 12],\n",
        "            'ring': [13, 14, 15, 16],\n",
        "            'pinky': [17, 18, 19, 20]\n",
        "        }\n",
        "\n",
        "        print(\"✅ Khởi tạo Sign Language Detector thành công!\")\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # LOAD RULES\n",
        "    # --------------------------------------------------\n",
        "    def load_rules(self, excel_path):\n",
        "        try:\n",
        "            df = pd.read_excel(excel_path)\n",
        "            rules = {}\n",
        "\n",
        "            for _, row in df.iterrows():\n",
        "                finger_name = str(row['Ten_bo_phan']).lower().strip()\n",
        "                gesture = str(row['Dong_tac']).lower().strip()\n",
        "                code = str(row['Ma_dong_tac']).strip()\n",
        "\n",
        "                if 'cái' in finger_name:\n",
        "                    finger_type = 'thumb'\n",
        "                elif 'trỏ' in finger_name:\n",
        "                    finger_type = 'index'\n",
        "                elif 'giữa' in finger_name:\n",
        "                    finger_type = 'middle'\n",
        "                elif 'áp út' in finger_name or 'ap ut' in finger_name:\n",
        "                    finger_type = 'ring'\n",
        "                elif 'út' in finger_name:\n",
        "                    finger_type = 'pinky'\n",
        "                else:\n",
        "                    continue\n",
        "\n",
        "                hand_side = 'left' if 'trái' in finger_name else 'right'\n",
        "                key = f\"{finger_type}_{hand_side}\"\n",
        "\n",
        "                if key not in rules:\n",
        "                    rules[key] = {}\n",
        "\n",
        "                rules[key][self.normalize_gesture_name(gesture)] = code\n",
        "\n",
        "            print(f\"✅ Đã load {len(rules)} nhóm quy tắc từ Excel\")\n",
        "            return rules\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Lỗi khi đọc Excel: {e}\")\n",
        "            return {}\n",
        "\n",
        "    def normalize_gesture_name(self, gesture):\n",
        "        return (\n",
        "            gesture.replace('duỗi', 'duoi')\n",
        "                   .replace('chạm', 'cham')\n",
        "                   .replace('út', 'ut')\n",
        "                   .strip()\n",
        "        )\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # UTILS\n",
        "    # --------------------------------------------------\n",
        "    def calculate_distance(self, p1, p2):\n",
        "        return math.sqrt(\n",
        "            (p1.x - p2.x) ** 2 +\n",
        "            (p1.y - p2.y) ** 2 +\n",
        "            (p1.z - p2.z) ** 2\n",
        "        )\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # FINGER DETECTION\n",
        "    # --------------------------------------------------\n",
        "    def detect_thumb_gesture(self, landmarks, hand_side):\n",
        "        thumb_tip = landmarks[4]\n",
        "        index_mcp = landmarks[5]\n",
        "\n",
        "        if hand_side == 'left':\n",
        "            return 'duoi' if thumb_tip.x < index_mcp.x else 'thu'\n",
        "        else:\n",
        "            return 'duoi' if thumb_tip.x > index_mcp.x else 'thu'\n",
        "\n",
        "    def detect_finger_gesture(self, landmarks, finger_type):\n",
        "        indices = self.finger_landmarks[finger_type]\n",
        "\n",
        "        wrist = landmarks[0]\n",
        "        mcp = landmarks[indices[0]]\n",
        "        tip = landmarks[indices[3]]\n",
        "\n",
        "        dist_tip = self.calculate_distance(wrist, tip)\n",
        "        dist_mcp = self.calculate_distance(wrist, mcp)\n",
        "\n",
        "        if dist_tip > dist_mcp + 0.02:\n",
        "            return 'duoi'\n",
        "        elif dist_tip < dist_mcp - 0.02:\n",
        "            return 'thu'\n",
        "        else:\n",
        "            return 'cong'\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # TOUCH DETECTION\n",
        "    # --------------------------------------------------\n",
        "    def detect_touching(self, landmarks, idx1, idx2, threshold=0.05):\n",
        "        return self.calculate_distance(landmarks[idx1], landmarks[idx2]) < threshold\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # ANALYZE HAND\n",
        "    # --------------------------------------------------\n",
        "    def analyze_hand(self, landmarks, hand_label):\n",
        "        hand_side = 'left' if hand_label == 'Left' else 'right'\n",
        "        gestures = {}\n",
        "\n",
        "        for finger, indices in self.finger_landmarks.items():\n",
        "            if finger == 'thumb':\n",
        "                gesture = self.detect_thumb_gesture(landmarks, hand_side)\n",
        "            else:\n",
        "                gesture = self.detect_finger_gesture(landmarks, finger)\n",
        "\n",
        "            rule_key = f\"{finger}_{hand_side}\"\n",
        "            if rule_key in self.rules and gesture in self.rules[rule_key]:\n",
        "                gestures[finger] = {\n",
        "                    'code': self.rules[rule_key][gesture],\n",
        "                    'gesture': gesture,\n",
        "                    'finger': finger,\n",
        "                    'hand': hand_side\n",
        "                }\n",
        "\n",
        "        return gestures\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # VIDEO PROCESS\n",
        "    # --------------------------------------------------\n",
        "    def process_video(self, video_path, output_path='output.json', show_video=False):\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "        results = []\n",
        "        frame_id = 0\n",
        "\n",
        "        while cap.isOpened():\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            res = self.hands.process(rgb)\n",
        "\n",
        "            frame_data = {\n",
        "                'frame': frame_id,\n",
        "                'timestamp': round(frame_id / fps, 2),\n",
        "                'gestures': []\n",
        "            }\n",
        "\n",
        "            if res.multi_hand_landmarks:\n",
        "                for i, hand_lm in enumerate(res.multi_hand_landmarks):\n",
        "                    label = res.multi_handedness[i].classification[0].label\n",
        "                    gestures = self.analyze_hand(hand_lm.landmark, label)\n",
        "\n",
        "                    for g in gestures.values():\n",
        "                        frame_data['gestures'].append(g)\n",
        "\n",
        "                    self.mp_drawing.draw_landmarks(\n",
        "                        frame, hand_lm, self.mp_hands.HAND_CONNECTIONS\n",
        "                    )\n",
        "\n",
        "            if frame_data['gestures']:\n",
        "                results.append(frame_data)\n",
        "\n",
        "            if show_video:\n",
        "                cv2.imshow(\"Sign Detection\", frame)\n",
        "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "                    break\n",
        "\n",
        "            frame_id += 1\n",
        "\n",
        "        cap.release()\n",
        "        cv2.destroyAllWindows()\n",
        "\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(results, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        print(f\"✅ Hoàn thành – phát hiện {len(results)} frame có gesture\")\n",
        "        return results\n",
        "\n",
        "\n",
        "# --------------------------------------------------\n",
        "# MAIN\n",
        "# --------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    detector = SignLanguageDetector('detech.xlsx')\n",
        "    detector.process_video('test_video.mp4', 'detected_gestures.json', show_video=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install mediapipe opencv-python pandas numpy\n"
      ],
      "metadata": {
        "id": "LQ3R0lytWotI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "import json\n",
        "from math import atan2, degrees\n",
        "\n",
        "# =======================\n",
        "# UTILS\n",
        "# =======================\n",
        "\n",
        "def dist(a, b):\n",
        "    return np.linalg.norm(np.array([a.x, a.y]) - np.array([b.x, b.y]))\n",
        "\n",
        "def angle(a, b, c):\n",
        "    ab = np.array([a.x - b.x, a.y - b.y])\n",
        "    cb = np.array([c.x - b.x, c.y - b.y])\n",
        "    rad = np.arccos(\n",
        "        np.dot(ab, cb) /\n",
        "        (np.linalg.norm(ab) * np.linalg.norm(cb) + 1e-6)\n",
        "    )\n",
        "    return degrees(rad)\n",
        "\n",
        "# =======================\n",
        "# FINGER DETECTOR\n",
        "# =======================\n",
        "\n",
        "class FingerGestureDetector:\n",
        "    finger_map = {\n",
        "        \"thumb\": [1, 2, 3, 4],\n",
        "        \"index\": [5, 6, 7, 8],\n",
        "        \"middle\": [9, 10, 11, 12],\n",
        "        \"ring\": [13, 14, 15, 16],\n",
        "        \"pinky\": [17, 18, 19, 20]\n",
        "    }\n",
        "\n",
        "    def detect_thumb(self, lm, hand):\n",
        "        tip = lm[4]\n",
        "        index_mcp = lm[5]\n",
        "\n",
        "        if hand == \"left\":\n",
        "            return \"duoi\" if tip.x < index_mcp.x else \"thu\"\n",
        "        else:\n",
        "            return \"duoi\" if tip.x > index_mcp.x else \"thu\"\n",
        "\n",
        "    def detect_finger(self, lm, finger):\n",
        "        ids = self.finger_map[finger]\n",
        "        wrist = lm[0]\n",
        "        mcp = lm[ids[0]]\n",
        "        tip = lm[ids[-1]]\n",
        "\n",
        "        if dist(wrist, tip) > dist(wrist, mcp) + 0.02:\n",
        "            return \"duoi\"\n",
        "        elif dist(wrist, tip) < dist(wrist, mcp) - 0.02:\n",
        "            return \"thu\"\n",
        "        else:\n",
        "            return \"cong\"\n",
        "\n",
        "    def analyze(self, lm, hand):\n",
        "        result = {}\n",
        "        for finger in self.finger_map:\n",
        "            if finger == \"thumb\":\n",
        "                result[finger] = self.detect_thumb(lm, hand)\n",
        "            else:\n",
        "                result[finger] = self.detect_finger(lm, finger)\n",
        "        return result\n",
        "\n",
        "# =======================\n",
        "# HAND POSE\n",
        "# =======================\n",
        "\n",
        "class HandPoseDetector:\n",
        "    def detect(self, lm):\n",
        "        wrist = lm[0]\n",
        "        index_mcp = lm[5]\n",
        "        pinky_mcp = lm[17]\n",
        "\n",
        "        dx = index_mcp.x - pinky_mcp.x\n",
        "        dy = index_mcp.y - pinky_mcp.y\n",
        "        ang = degrees(atan2(dy, dx))\n",
        "\n",
        "        if abs(ang) < 20:\n",
        "            return \"sap_thang\"\n",
        "        elif ang > 20:\n",
        "            return \"nghieng_phai\"\n",
        "        else:\n",
        "            return \"nghieng_trai\"\n",
        "\n",
        "# =======================\n",
        "# ARM DETECTOR\n",
        "# =======================\n",
        "\n",
        "class ArmAnalyzer:\n",
        "    def detect(self, shoulder, elbow, wrist):\n",
        "        ang = angle(shoulder, elbow, wrist)\n",
        "\n",
        "        if ang > 160:\n",
        "            return \"duoi_thang\"\n",
        "        elif ang > 90:\n",
        "            return \"nghieng\"\n",
        "        else:\n",
        "            return \"co\"\n",
        "\n",
        "# =======================\n",
        "# BODY POSITION\n",
        "# =======================\n",
        "\n",
        "class BodyPositionAnalyzer:\n",
        "    def detect(self, wrist, nose):\n",
        "        if wrist.y < nose.y - 0.05:\n",
        "            return \"tren\"\n",
        "        elif wrist.y > nose.y + 0.05:\n",
        "            return \"duoi\"\n",
        "        else:\n",
        "            return \"giua\"\n",
        "\n",
        "# =======================\n",
        "# MAIN DETECTOR\n",
        "# =======================\n",
        "\n",
        "class SignLanguageDetector:\n",
        "    def __init__(self):\n",
        "        self.mp_holistic = mp.solutions.holistic\n",
        "        self.holistic = self.mp_holistic.Holistic(\n",
        "            min_detection_confidence=0.5,\n",
        "            min_tracking_confidence=0.5\n",
        "        )\n",
        "\n",
        "        self.finger = FingerGestureDetector()\n",
        "        self.hand_pose = HandPoseDetector()\n",
        "        self.arm = ArmAnalyzer()\n",
        "        self.body_pos = BodyPositionAnalyzer()\n",
        "\n",
        "    def process_frame(self, frame):\n",
        "        img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        res = self.holistic.process(img)\n",
        "\n",
        "        output = []\n",
        "\n",
        "        if res.pose_landmarks:\n",
        "            pose = res.pose_landmarks.landmark\n",
        "            nose = pose[self.mp_holistic.PoseLandmark.NOSE]\n",
        "\n",
        "            # LEFT HAND\n",
        "            if res.left_hand_landmarks:\n",
        "                lm = res.left_hand_landmarks.landmark\n",
        "                fingers = self.finger.analyze(lm, \"left\")\n",
        "                pose_hand = self.hand_pose.detect(lm)\n",
        "\n",
        "                arm_pose = self.arm.detect(\n",
        "                    pose[self.mp_holistic.PoseLandmark.LEFT_SHOULDER],\n",
        "                    pose[self.mp_holistic.PoseLandmark.LEFT_ELBOW],\n",
        "                    pose[self.mp_holistic.PoseLandmark.LEFT_WRIST]\n",
        "                )\n",
        "\n",
        "                body_pos = self.body_pos.detect(\n",
        "                    pose[self.mp_holistic.PoseLandmark.LEFT_WRIST],\n",
        "                    nose\n",
        "                )\n",
        "\n",
        "                output.append({\n",
        "                    \"hand\": \"left\",\n",
        "                    \"fingers\": fingers,\n",
        "                    \"hand_pose\": pose_hand,\n",
        "                    \"arm\": arm_pose,\n",
        "                    \"body_position\": body_pos\n",
        "                })\n",
        "\n",
        "            # RIGHT HAND\n",
        "            if res.right_hand_landmarks:\n",
        "                lm = res.right_hand_landmarks.landmark\n",
        "                fingers = self.finger.analyze(lm, \"right\")\n",
        "                pose_hand = self.hand_pose.detect(lm)\n",
        "\n",
        "                arm_pose = self.arm.detect(\n",
        "                    pose[self.mp_holistic.PoseLandmark.RIGHT_SHOULDER],\n",
        "                    pose[self.mp_holistic.PoseLandmark.RIGHT_ELBOW],\n",
        "                    pose[self.mp_holistic.PoseLandmark.RIGHT_WRIST]\n",
        "                )\n",
        "\n",
        "                body_pos = self.body_pos.detect(\n",
        "                    pose[self.mp_holistic.PoseLandmark.RIGHT_WRIST],\n",
        "                    nose\n",
        "                )\n",
        "\n",
        "                output.append({\n",
        "                    \"hand\": \"right\",\n",
        "                    \"fingers\": fingers,\n",
        "                    \"hand_pose\": pose_hand,\n",
        "                    \"arm\": arm_pose,\n",
        "                    \"body_position\": body_pos\n",
        "                })\n",
        "\n",
        "        return output\n",
        "\n",
        "# =======================\n",
        "# VIDEO PROCESS\n",
        "# =======================\n",
        "\n",
        "def process_video(video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    detector = SignLanguageDetector()\n",
        "\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    frame_id = 0\n",
        "    results = []\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        gestures = detector.process_frame(frame)\n",
        "        if gestures:\n",
        "            results.append({\n",
        "                \"frame\": frame_id,\n",
        "                \"timestamp\": round(frame_id / fps, 2),\n",
        "                \"gestures\": gestures\n",
        "            })\n",
        "\n",
        "        frame_id += 1\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    with open(\"detected_gestures.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(\"✅ DONE – detected_gestures.json\")\n",
        "\n",
        "# =======================\n",
        "# RUN\n",
        "# =======================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    process_video(\"test_video.mp4\")\n"
      ],
      "metadata": {
        "id": "TqsVMt-nTDgp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}